import os
import json
import requests
import streamlit as st
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_groq import ChatGroq
from langchain.chains import ConversationalRetrievalChain

# Thi·∫øt l·∫≠p giao di·ªán Streamlit
st.set_page_config(page_title="Chat with Llama-3.3", page_icon="ü§ñ", layout="centered")

# ƒê∆∞·ªùng d·∫´n API Groq
GROQ_API_BASE = "https://api.groq.com/openai/v1"

# H√†m t·∫£i API Key
def load_api_key():
    try:
        with open("config.json") as f:
            config_data = json.load(f)
        api_key = config_data.get("GROQ_API_KEY")
        if not api_key:
            raise ValueError("API Key kh√¥ng t·ªìn t·∫°i. Vui l√≤ng th√™m v√†o config.json.")
        os.environ["GROQ_API_KEY"] = api_key
        return api_key
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i API Key: {str(e)}")
        st.stop()

# Kh·ªüi t·∫°o Vectorstore
@st.cache_resource
def setup_vectorstore():
    try:
        persist_directory = "vector_db_nlp"
        embeddings = HuggingFaceEmbeddings()
        st.info("Loading data and initializing Vectorstore...")
        vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
        return vectorstore
    except Exception as e:
        st.error(f"L·ªói khi kh·ªüi t·∫°o Vectorstore: {str(e)}")
        st.stop()

# T·∫°o pipeline truy v·∫•n
def chat_chain(vectorstore):
    try:
        llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0.2)
        retriever = vectorstore.as_retriever()
        chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            return_source_documents=True,
            output_key="answer"  # Ch·ªâ ƒë·ªãnh kh√≥a tr·∫£ v·ªÅ
        )
        return chain
    except Exception as e:
        st.error(f"L·ªói khi kh·ªüi t·∫°o pipeline: {str(e)}")
        st.stop()

# T·∫£i API Key v√† kh·ªüi t·∫°o h·ªá th·ªëng
api_key = load_api_key()

if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = setup_vectorstore()

if "chat_chain" not in st.session_state:
    st.session_state.chat_chain = chat_chain(st.session_state.vectorstore)

# Giao di·ªán Streamlit
st.title("üìö PDF Document QA System with LLaMA 3.3")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []  # L·ªãch s·ª≠ h·ªôi tho·∫°i tr·ªëng ban ƒë·∫ßu

for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

user_input = st.chat_input("Enter your question here!")
if user_input:
    with st.chat_message("user"):
        st.markdown(user_input)
        st.session_state.chat_history.append({"role": "user", "content": user_input})

    try:
        # G·ªçi pipeline v·ªõi chat_history v√† c√¢u h·ªèi
        response = st.session_state.chat_chain.invoke({
            "question": user_input,
            "chat_history": [msg["content"] for msg in st.session_state.chat_history if msg["role"] == "assistant"]
        })
        assistant_response = response["answer"]

        with st.chat_message("assistant"):
            st.markdown(assistant_response)
            st.session_state.chat_history.append({"role": "assistant", "content": assistant_response})

    except Exception as e:
        st.error(f"L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}")
